# -*- coding: utf-8 -*-
"""live_candle_stick_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SqJY7Eahvc4flMWHBr3vupyOiDXL0xNn
"""

import yfinance as yf
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.sql import SparkSession
from pyspark.ml.classification import LogisticRegressionModel
from pyspark.sql.window import Window
from pyspark.sql.functions import lag, col,when,lead
import zipfile
import os
import time
import pandas as pd
from datetime import timedelta


# Example: get 1-minute data for AAPL for the last 5 days
data = yf.download("GLD", interval="1m", period="1d")



spark = SparkSession.builder.appName("LoadModel").getOrCreate()

if not data.empty:
    # Simplify column names for easier access
    data.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in data.columns.values]

    # 1. Calculate the Typical Price (TP)
    data['Typical Price'] = (data['High_GLD'] + data['Low_GLD'] + data['Close_GLD']) / 3

    # 2. Multiply TP by Volume to get Price-Volume (PV)
    data['Price-Volume'] = data['Typical Price'] * data['Volume_GLD']

    # 3. Calculate the Cumulative PV and Cumulative Volume
    data['Cumulative PV'] = data['Price-Volume'].cumsum()
    data['Cumulative Volume'] = data['Volume_GLD'].cumsum()

    # 4. Calculate VWAP by dividing Cumulative PV by Cumulative Volume
    data['VWAP'] = data['Cumulative PV'] / data['Cumulative Volume']

    # Display the minute-by-minute high, low, volume, and VWAP
    print(data[['High_GLD', 'Low_GLD', 'Volume_GLD', 'VWAP']].tail())
else:
    print("Could not retrieve minute-by-minute data to calculate VWAP.")

last_5 = data.tail(10).reset_index()
last_5.rename(columns={'Datetime':'timestamp'}, inplace=True)
last_1 = data.tail(1).reset_index()
last_1.rename(columns={'Datetime':'timestamp'}, inplace=True)

spark_df = spark.createDataFrame(last_5)

spark_df = spark_df.drop("Typical Price_", "Price-Volume_","Cumulative PV_","Cumulative Volume_","VWAP_",'Typical Price',"Price-Volume","Cumulative PV","Cumulative Volume")

windowSpec = Window.orderBy("timestamp")

for i in range(10):
  spark_df = spark_df.withColumn(
    f"open{i}",
    lead("Open_GLD", i).over(windowSpec)
)

  spark_df = spark_df.withColumn(
    f"high{i}",
    lead("High_GLD", i).over(windowSpec)
)
  spark_df = spark_df.withColumn(
    f"low{i}",
    lead("Low_GLD", i).over(windowSpec)
)
  spark_df = spark_df.withColumn(
    f"close{i}",
    lead("Close_GLD", i).over(windowSpec)
)
  spark_df = spark_df.withColumn(
    f"VWAP{i}",
    lead("VWAP", i).over(windowSpec)
)





spark_df = spark_df.filter(spark_df["vwap9"].isNotNull())
spark_df = spark_df.drop('timestamp')


# Dictionary mapping old column names to new column names
column_mapping = {
    "Close_GLD": 'close',
    "High_GLD": "high",
    "Low_GLD": "low",
    "Open_GLD": 'open',
    "Volume_GLD": "volume",
    "VWAP": 'vwap'
}

# Rename columns using a loop
for old_name, new_name in column_mapping.items():
    spark_df = spark_df.withColumnRenamed(old_name, new_name)



org_open = spark_df.select('open').first()[0]

selected_cols = spark_df.columns

feature_cols = selected_cols

assembler = VectorAssembler(
    inputCols=feature_cols,
    outputCol="features"
)
spark_df = assembler.transform(spark_df)

zip_path = "model_lr_5m.zip"  # Replace with your uploaded zip filename
extract_path = "saved_model"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Model extracted to:", extract_path)
print("Contents:", os.listdir(extract_path))

# Path to the extracted folder
model_path = os.path.join(extract_path, "/content/saved_model/content/model_lr_5m")  # folder inside zip

# Load the model
loaded_model = LogisticRegressionModel.load(model_path)

predictions = loaded_model.transform(spark_df)
predictions.select("prediction", "probability").show()
pred_var = predictions.select("prediction").first()[0]

# Wait 5 minutes
time.sleep(5 * 60)

# Pandas: Add 5 minutes to timestamp
last_1["new_time"] = last_1["timestamp"] + timedelta(minutes=5)

# Extract predicted time
pred_time = last_1["new_time"].iloc[0]

# Fetch live 1-minute data
data = yf.download("GLD", interval="1m", period="1d").reset_index()

# Find matching timestamp
accu = data[data["Datetime"] == pred_time]

# If no exact match (time mismatch), pick nearest row
if accu.empty:
    accu = data.iloc[(data["Datetime"] - pred_time).abs().argsort()[:1]]

close_price = float(accu["Close"].values[0])

if close_price > org_open and pred_var == 1:
    print("✅ Correct prediction (BUY)")
elif close_price < org_open and pred_var == 0:
    print("✅ Correct prediction (SELL)")
else:
    print("❌ Wrong prediction")
print("open time:",org_open)
print("closed price:",close_price)
print('prediction value:',pred_var)
