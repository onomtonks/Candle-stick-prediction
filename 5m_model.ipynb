{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -y\n",
        "!apt-get install openjdk-11-jdk -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrvFZh5kbhP2",
        "outputId": "8d66b271-0280-47d0-feac-6335cf278ffa"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,818 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,404 kB]\n",
            "Fetched 12.2 MB in 4s (2,873 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-11-jdk is already the newest version (11.0.28+6-1ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "HiT7JM_kaypv"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, LongType, IntegerType, StringType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the file line by line\n",
        "with open(\"gold_data_for_2024_2025.txt\", \"r\") as file:\n",
        "    raw_data = file.read().splitlines()  # or use: list(file)\n",
        "\n",
        "data = []\n",
        "for s in raw_data:\n",
        "    # Skip empty lines\n",
        "    if not s.strip():\n",
        "        continue\n",
        "\n",
        "    # Extract key=value pairs\n",
        "    items = re.findall(r'(\\w+)=([-\\w.]+|None)', s)\n",
        "\n",
        "    # Convert to dict\n",
        "    d = {}\n",
        "    for k, v in items:\n",
        "        if v == \"None\":\n",
        "            d[k] = None\n",
        "        else:\n",
        "            try:\n",
        "                d[k] = float(v)\n",
        "            except ValueError:\n",
        "                d[k] = v\n",
        "    data.append(d)\n",
        "\n",
        "# Create PySpark DataFrame\n",
        "spark = SparkSession.builder.appName(\"GoldData\").getOrCreate()\n",
        "schema = StructType([\n",
        "    StructField(\"open\", DoubleType(), True),\n",
        "    StructField(\"high\", DoubleType(), True),\n",
        "    StructField(\"low\", DoubleType(), True),\n",
        "    StructField(\"close\", DoubleType(), True),\n",
        "    StructField(\"volume\", DoubleType(), True),        # was LongType()\n",
        "    StructField(\"vwap\", DoubleType(), True),\n",
        "    StructField(\"timestamp\", DoubleType(), True),     # was LongType()\n",
        "    StructField(\"transactions\", DoubleType(), True), # was IntegerType()\n",
        "    StructField(\"otc\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Then create the DataFrame with the schema\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQPU0aNNbOGO",
        "outputId": "63d49c31-67fc-4314-fe62-628e860dd4d8"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+------+------+------+--------+-------------+------------+----+\n",
            "|  open|  high|   low| close|volume|    vwap|    timestamp|transactions| otc|\n",
            "+------+------+------+------+------+--------+-------------+------------+----+\n",
            "|191.61| 192.0|191.61| 192.0| 616.0|191.6739|  1.704186E12|         4.0|NULL|\n",
            "| 192.2| 192.2| 192.2| 192.2| 515.0|   192.2|1.70418624E12|         6.0|NULL|\n",
            "|192.25|192.25|192.25|192.25| 500.0|  192.25| 1.7041866E12|         1.0|NULL|\n",
            "| 192.3| 192.3| 192.3| 192.3| 100.0|   192.3|1.70418666E12|         1.0|NULL|\n",
            "| 192.3| 192.3| 192.3| 192.3| 250.0|   192.3|1.70418702E12|         3.0|NULL|\n",
            "|192.32|192.32|192.26|192.26|3010.0|192.2997|1.70418768E12|        12.0|NULL|\n",
            "| 192.6| 192.6| 192.6| 192.6| 300.0|   192.6|1.70418882E12|         1.0|NULL|\n",
            "|192.26|192.26|192.26|192.26| 201.0|192.2702|1.70419038E12|         4.0|NULL|\n",
            "|192.23|192.23|192.23|192.23| 598.0|  192.23|1.70419068E12|         1.0|NULL|\n",
            "| 192.3| 192.3| 192.3| 192.3| 101.0|192.3002|1.70419146E12|         2.0|NULL|\n",
            "|192.15|192.15|192.14|192.14| 647.0|192.1522| 1.7041944E12|         9.0|NULL|\n",
            "| 192.3| 192.3| 192.3| 192.3| 510.0|192.2994|  1.704195E12|         2.0|NULL|\n",
            "|191.97|191.97|191.97|191.97| 507.0|191.9767|1.70419524E12|         8.0|NULL|\n",
            "|191.86|191.86|191.81|191.81|5403.0|  191.82| 1.7041953E12|        37.0|NULL|\n",
            "|191.87|191.87|191.87|191.87|1008.0|191.8712|1.70419536E12|         5.0|NULL|\n",
            "|191.93|191.93|191.93|191.93| 185.0|191.9203|1.70419548E12|         4.0|NULL|\n",
            "|191.92|191.92|191.92|191.92| 100.0|  191.92|1.70419554E12|         1.0|NULL|\n",
            "| 191.7| 191.7| 191.7| 191.7|2115.0|191.6977|1.70419584E12|        10.0|NULL|\n",
            "|191.68| 191.7|191.68| 191.7|1279.0|191.6894|1.70419686E12|         4.0|NULL|\n",
            "|191.53|191.53|191.53|191.53| 500.0|  191.53|1.70419704E12|         1.0|NULL|\n",
            "+------+------+------+------+------+--------+-------------+------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_5m = df.drop(\"otc\",'transactions')"
      ],
      "metadata": {
        "id": "RSCqjMYlcAJE"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import lag, col,when,lead"
      ],
      "metadata": {
        "id": "D3plSYFucJh8"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "windowSpec = Window.orderBy(\"timestamp\")"
      ],
      "metadata": {
        "id": "pQ2hT-Fcc3qO"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EPAhao65dAG_"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  df_5m = df_5m.withColumn(\n",
        "    f\"open{i}\",\n",
        "    lead(\"open\", i).over(windowSpec)\n",
        ")\n",
        "\n",
        "  df_5m = df_5m.withColumn(\n",
        "    f\"high{i}\",\n",
        "    lead(\"high\", i).over(windowSpec)\n",
        ")\n",
        "  df_5m = df_5m.withColumn(\n",
        "    f\"low{i}\",\n",
        "    lead(\"low\", i).over(windowSpec)\n",
        ")\n",
        "  df_5m = df_5m.withColumn(\n",
        "    f\"close{i}\",\n",
        "    lead(\"close\", i).over(windowSpec)\n",
        ")\n",
        "  df_5m = df_5m.withColumn(\n",
        "    f\"vwap{i}\",\n",
        "    lead(\"vwap\", i).over(windowSpec)\n",
        ")\n",
        "\n",
        "df_5m = df_5m.withColumn(\n",
        "    \"BUY/SELL\",\n",
        "    when(lead(\"close\", 15).over(windowSpec)>col('open'),1).otherwise(0)\n",
        ")\n"
      ],
      "metadata": {
        "id": "Bzp_GGBbcWr3"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import row_number, desc"
      ],
      "metadata": {
        "id": "pJueFflYe2HB"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = Window.orderBy(desc(\"timestamp\"))   # replace with your ordering column\n",
        "df_numbered = df_5m.withColumn(\"row_num\", row_number().over(w))\n",
        "\n",
        "# Keep everything except last 5\n",
        "df_5m = df_numbered.filter(\"row_num > 10\").drop(\"row_num\")\n"
      ],
      "metadata": {
        "id": "GQB_38jsep4J"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in df_5m.tail(5):\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEJJB5jid5Jn",
        "outputId": "50f5eb38-174d-42aa-ef99-44a6c41ef8c1"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(open=192.3, high=192.3, low=192.3, close=192.3, volume=250.0, vwap=192.3, timestamp=1704187020000.0, open0=192.3, high0=192.3, low0=192.3, close0=192.3, vwap0=192.3, open1=192.32, high1=192.32, low1=192.26, close1=192.26, vwap1=192.2997, open2=192.6, high2=192.6, low2=192.6, close2=192.6, vwap2=192.6, open3=192.26, high3=192.26, low3=192.26, close3=192.26, vwap3=192.2702, open4=192.23, high4=192.23, low4=192.23, close4=192.23, vwap4=192.23, open5=192.3, high5=192.3, low5=192.3, close5=192.3, vwap5=192.3002, open6=192.15, high6=192.15, low6=192.14, close6=192.14, vwap6=192.1522, open7=192.3, high7=192.3, low7=192.3, close7=192.3, vwap7=192.2994, open8=191.97, high8=191.97, low8=191.97, close8=191.97, vwap8=191.9767, open9=191.86, high9=191.86, low9=191.81, close9=191.81, vwap9=191.82, BUY/SELL=0)\n",
            "Row(open=192.3, high=192.3, low=192.3, close=192.3, volume=100.0, vwap=192.3, timestamp=1704186660000.0, open0=192.3, high0=192.3, low0=192.3, close0=192.3, vwap0=192.3, open1=192.3, high1=192.3, low1=192.3, close1=192.3, vwap1=192.3, open2=192.32, high2=192.32, low2=192.26, close2=192.26, vwap2=192.2997, open3=192.6, high3=192.6, low3=192.6, close3=192.6, vwap3=192.6, open4=192.26, high4=192.26, low4=192.26, close4=192.26, vwap4=192.2702, open5=192.23, high5=192.23, low5=192.23, close5=192.23, vwap5=192.23, open6=192.3, high6=192.3, low6=192.3, close6=192.3, vwap6=192.3002, open7=192.15, high7=192.15, low7=192.14, close7=192.14, vwap7=192.1522, open8=192.3, high8=192.3, low8=192.3, close8=192.3, vwap8=192.2994, open9=191.97, high9=191.97, low9=191.97, close9=191.97, vwap9=191.9767, BUY/SELL=0)\n",
            "Row(open=192.25, high=192.25, low=192.25, close=192.25, volume=500.0, vwap=192.25, timestamp=1704186600000.0, open0=192.25, high0=192.25, low0=192.25, close0=192.25, vwap0=192.25, open1=192.3, high1=192.3, low1=192.3, close1=192.3, vwap1=192.3, open2=192.3, high2=192.3, low2=192.3, close2=192.3, vwap2=192.3, open3=192.32, high3=192.32, low3=192.26, close3=192.26, vwap3=192.2997, open4=192.6, high4=192.6, low4=192.6, close4=192.6, vwap4=192.6, open5=192.26, high5=192.26, low5=192.26, close5=192.26, vwap5=192.2702, open6=192.23, high6=192.23, low6=192.23, close6=192.23, vwap6=192.23, open7=192.3, high7=192.3, low7=192.3, close7=192.3, vwap7=192.3002, open8=192.15, high8=192.15, low8=192.14, close8=192.14, vwap8=192.1522, open9=192.3, high9=192.3, low9=192.3, close9=192.3, vwap9=192.2994, BUY/SELL=0)\n",
            "Row(open=192.2, high=192.2, low=192.2, close=192.2, volume=515.0, vwap=192.2, timestamp=1704186240000.0, open0=192.2, high0=192.2, low0=192.2, close0=192.2, vwap0=192.2, open1=192.25, high1=192.25, low1=192.25, close1=192.25, vwap1=192.25, open2=192.3, high2=192.3, low2=192.3, close2=192.3, vwap2=192.3, open3=192.3, high3=192.3, low3=192.3, close3=192.3, vwap3=192.3, open4=192.32, high4=192.32, low4=192.26, close4=192.26, vwap4=192.2997, open5=192.6, high5=192.6, low5=192.6, close5=192.6, vwap5=192.6, open6=192.26, high6=192.26, low6=192.26, close6=192.26, vwap6=192.2702, open7=192.23, high7=192.23, low7=192.23, close7=192.23, vwap7=192.23, open8=192.3, high8=192.3, low8=192.3, close8=192.3, vwap8=192.3002, open9=192.15, high9=192.15, low9=192.14, close9=192.14, vwap9=192.1522, BUY/SELL=0)\n",
            "Row(open=191.61, high=192.0, low=191.61, close=192.0, volume=616.0, vwap=191.6739, timestamp=1704186000000.0, open0=191.61, high0=192.0, low0=191.61, close0=192.0, vwap0=191.6739, open1=192.2, high1=192.2, low1=192.2, close1=192.2, vwap1=192.2, open2=192.25, high2=192.25, low2=192.25, close2=192.25, vwap2=192.25, open3=192.3, high3=192.3, low3=192.3, close3=192.3, vwap3=192.3, open4=192.3, high4=192.3, low4=192.3, close4=192.3, vwap4=192.3, open5=192.32, high5=192.32, low5=192.26, close5=192.26, vwap5=192.2997, open6=192.6, high6=192.6, low6=192.6, close6=192.6, vwap6=192.6, open7=192.26, high7=192.26, low7=192.26, close7=192.26, vwap7=192.2702, open8=192.23, high8=192.23, low8=192.23, close8=192.23, vwap8=192.23, open9=192.3, high9=192.3, low9=192.3, close9=192.3, vwap9=192.3002, BUY/SELL=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
      ],
      "metadata": {
        "id": "_CpdmpH5fD7_"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evalutations(predictions):\n",
        "    multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"BUY/SELL\", predictionCol=\"prediction\")\n",
        "    binary_evaluator = BinaryClassificationEvaluator(labelCol=\"BUY/SELL\", rawPredictionCol=\"rawPrediction\")\n",
        "\n",
        "    accuracy = multi_evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
        "    f1 = multi_evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
        "    precision = multi_evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
        "    recall = multi_evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
        "    auc = binary_evaluator.setMetricName(\"areaUnderROC\").evaluate(predictions)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "    return predictions.groupBy(\"BUY/SELL\", \"prediction\").count()"
      ],
      "metadata": {
        "id": "m-LGkgOEgzZs"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example columns\n",
        "excluded_col = [\"timestamp\",\"transactions\",\"BUY/SELL\"]\n",
        "selected_cols = [c for c in df_5m.columns if c not in excluded_col]\n",
        "\n",
        "feature_cols = selected_cols\n",
        "\n",
        "# Convert the target label to numeric\n",
        "df_5m = df_5m.withColumn(\"BUY/SELL\", col(\"BUY/SELL\").cast(\"integer\"))"
      ],
      "metadata": {
        "id": "LZ_8ih-gfNxr"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols,\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "df_5m = assembler.transform(df_5m)"
      ],
      "metadata": {
        "id": "mc0zN9OTfcZ3"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = df_5m.randomSplit([0.7, 0.3], seed=42)"
      ],
      "metadata": {
        "id": "-ywut_4jf3YE"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzjelJzPQU8V",
        "outputId": "7b5ca20c-4ad1-455d-f040-837c137f41df"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['open',\n",
              " 'high',\n",
              " 'low',\n",
              " 'close',\n",
              " 'volume',\n",
              " 'vwap',\n",
              " 'open0',\n",
              " 'high0',\n",
              " 'low0',\n",
              " 'close0',\n",
              " 'vwap0',\n",
              " 'open1',\n",
              " 'high1',\n",
              " 'low1',\n",
              " 'close1',\n",
              " 'vwap1',\n",
              " 'open2',\n",
              " 'high2',\n",
              " 'low2',\n",
              " 'close2',\n",
              " 'vwap2',\n",
              " 'open3',\n",
              " 'high3',\n",
              " 'low3',\n",
              " 'close3',\n",
              " 'vwap3',\n",
              " 'open4',\n",
              " 'high4',\n",
              " 'low4',\n",
              " 'close4',\n",
              " 'vwap4',\n",
              " 'open5',\n",
              " 'high5',\n",
              " 'low5',\n",
              " 'close5',\n",
              " 'vwap5',\n",
              " 'open6',\n",
              " 'high6',\n",
              " 'low6',\n",
              " 'close6',\n",
              " 'vwap6',\n",
              " 'open7',\n",
              " 'high7',\n",
              " 'low7',\n",
              " 'close7',\n",
              " 'vwap7',\n",
              " 'open8',\n",
              " 'high8',\n",
              " 'low8',\n",
              " 'close8',\n",
              " 'vwap8',\n",
              " 'open9',\n",
              " 'high9',\n",
              " 'low9',\n",
              " 'close9',\n",
              " 'vwap9']"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with any null values in the feature columns before grouping\n",
        "train_df = train_df.na.drop(subset=feature_cols)\n",
        "train_df.groupBy(\"BUY/SELL\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzL5giiHfhH6",
        "outputId": "1ea953a6-40db-4a38-c61d-13a4d4cbf336"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|BUY/SELL|count|\n",
            "+--------+-----+\n",
            "|       1|46737|\n",
            "|       0|44463|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with any null values in the feature columns from test DataFrame before making predictions\n",
        "test_df = test_df.na.drop(subset=feature_cols)\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"BUY/SELL\")\n",
        "model = lr.fit(train_df)\n",
        "predictions = model.transform(test_df)"
      ],
      "metadata": {
        "id": "3gTY-AnmgZvZ"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalutations(predictions).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBfTlACKg7Wa",
        "outputId": "4d00f9d6-e8b9-4b23-bce9-114bfe4cbc65"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7837\n",
            "F1 Score: 0.7836\n",
            "Precision: 0.7838\n",
            "Recall: 0.7837\n",
            "AUC: 0.8579\n",
            "+--------+----------+-----+\n",
            "|BUY/SELL|prediction|count|\n",
            "+--------+----------+-----+\n",
            "|       1|       1.0|15970|\n",
            "|       1|       0.0| 3989|\n",
            "|       0|       0.0|14779|\n",
            "|       0|       1.0| 4499|\n",
            "+--------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model_lr_5m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "PEcWd_Tebk2A",
        "outputId": "38d0225b-52fd-4cc0-e644-99a7a1d042ad"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o4504.save.\n: java.io.IOException: Path model_lr_5m already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1687682742.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_lr_5m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a string, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"JavaMLWriter\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4504.save.\n: java.io.IOException: Path model_lr_5m already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "   !zip -r /content/model_lr_5m.zip /content/model_lr_5m"
      ],
      "metadata": {
        "id": "oolB2pkpbvPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.select(\"BUY/SELL\", \"prediction\", \"probability\").show(10, truncate=False)"
      ],
      "metadata": {
        "id": "8jhmQ0NliZG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"BUY/SELL\", numTrees=100)\n",
        "model = rf.fit(train_df)\n",
        "predictions = model.transform(test_df)\n",
        "evalutations(predictions).show()"
      ],
      "metadata": {
        "id": "Bz54I0MIT7E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"BUY/SELL\", maxIter=50)\n",
        "model = gbt.fit(train_df)\n",
        "predictions = model.transform(test_df)\n",
        "predictions.select(\"BUY/SELL\", \"prediction\", \"probability\").show(10, truncate=False)\n"
      ],
      "metadata": {
        "id": "LhpV4QuEVAqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalutations(predictions).show()"
      ],
      "metadata": {
        "id": "owQTwaubUgBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UemA8vHlVup0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}